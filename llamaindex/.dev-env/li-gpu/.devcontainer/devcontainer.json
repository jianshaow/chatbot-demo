{
	"name": "LI-GPU",
	"image": "jianshao/llamaindex-demo:0.12.19",
	"remoteEnv": {
		"PYTHONPATH": ".",
		"OLLAMA_HOST": "host.docker.internal",
		"LLAMA_INDEX_CACHE_DIR": "/home/devel/.cache/huggingface/hub"
	},
	"runArgs": [
		"--gpus=all",
		"--add-host=host.docker.internal:host-gateway"
	],
	"mounts": [
		{
			"source": "${localEnv:HOME}/.config",
			"target": "/home/devel/.config",
			"type": "bind"
		},
		{
			"source": "${localEnv:HOME}/.cache",
			"target": "/home/devel/.cache",
			"type": "bind"
		},
		{
			"source": "${localEnv:HOME}/.ssh",
			"target": "/home/devel/.ssh",
			"type": "bind"
		}
	],
	"customizations": {
		"vscode": {
			"extensions": [
				"ms-python.python",
				"ms-python.black-formatter",
				"ms-python.pylint",
				"ms-python.isort"
			]
		}
	}
}