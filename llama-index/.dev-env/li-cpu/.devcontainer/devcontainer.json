{
	"name": "LI-CPU",
	"image": "jianshao/llamaindex-demo:0.10.61-cpu",
	"remoteEnv": {
		"PYTHONPATH": ".",
		"OLLAMA_BASE_URL": "http://host.docker.internal:11434",
		"OLLAMA_CHAT_MODEL": "qwen2:1.5b",
		"HF_CHAT_MODEL": "google/gemma-1.1-2b-it",
		"LLAMA_INDEX_CACHE_DIR": "/home/devel/.cache/huggingface/hub"
	},
	"runArgs": [
		"--add-host=host.docker.internal:host-gateway"
	],
	"mounts": [
		{
			"source": "${localEnv:HOME}/.config",
			"target": "/home/devel/.config",
			"type": "bind"
		},
		{
			"source": "${localEnv:HOME}/.cache",
			"target": "/home/devel/.cache",
			"type": "bind"
		},
		{
			"source": "${localEnv:HOME}/.ssh",
			"target": "/home/devel/.ssh",
			"type": "bind"
		}
	],
	"customizations": {
		"vscode": {
			"extensions": [
				"ms-python.python",
				"ms-python.vscode-pylance",
				"ms-python.black-formatter",
				"ms-python.isort"
			]
		}
	}
}